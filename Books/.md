---
tag: "\U0001F4DABook"
aliases:
  - 図解即戦力　機械学習&ディープラーニングのしくみと技術がこれ1冊でしっかりわかる教科書 - 株式会社アイデミー 山口達輝、株式会社アイデミー 松田洋之
status: unread
Author: 株式会社アイデミー 山口達輝、株式会社アイデミー 松田洋之
Cover: 'https://m.media-amazon.com/images/I/81vU3g+ZDVL._SY160.jpg'
Publisher: null
source: Kindle / Paper
Type: Techs / Others
kindle-sync:
  bookId: '15522'
  title: 図解即戦力　機械学習&ディープラーニングのしくみと技術がこれ1冊でしっかりわかる教科書
  author: 株式会社アイデミー 山口達輝、株式会社アイデミー 松田洋之
  asin: B07X1WL8XN
  lastAnnotatedDate: '2025-07-12'
  bookImageUrl: 'https://m.media-amazon.com/images/I/81vU3g+ZDVL._SY160.jpg'
  highlightsCount: 52
---
# 図解即戦力　機械学習&ディープラーニングのしくみと技術がこれ1冊でしっかりわかる教科書
## Metadata
* [[株式会社アイデミー 山口達輝、株式会社アイデミー 松田洋之]]
![image|150](https://m.media-amazon.com/images/I/81vU3g+ZDVL._SY160.jpg)
* ASIN: B07X1WL8XN
* Reference: https://www.amazon.com/dp/B07X1WL8XN
* [Kindle link](kindle://book?action=open&asin=B07X1WL8XN)
Started: 
Finished: 
## 要約
## 読書メモ
## Highlights
>[!quote]
>機械学習の問題は、大きく 分類 と 回帰 に分けられます。 — location: [177](kindle://book?action=open&asin=B07X1WL8XN&location=177) ^ref-35122
>>[!note]
>ここスーパー基礎、大事ポイント。
---
>[!quote]
>グラフ上にデータがプロット（書き込むこと）されているとすれば、分類はデータ全体をできるだけ分けるように線を引くこと、回帰はデータ全体にできるだけ重なるように線を引くことに相当します。 — location: [182](kindle://book?action=open&asin=B07X1WL8XN&location=182) ^ref-46804
>>[!note]
>概念を掴む。この違いを捉えて忘れないで。
---
>[!quote]
>隠れ層とは「入力層から受け取った情報をさまざまな組み合わせで伝えていき、出力層に役立つ形に情報を変形して渡す」という役割を持つ層のことです。 — location: [218](kindle://book?action=open&asin=B07X1WL8XN&location=218) ^ref-41267
>>[!note]
>ディープラーニングの特徴とのことだが、いまいち理解できていない。
---
>[!quote]
>ディープラーニングが画期的だったのは、 最適な特徴量を自動的に抽出 するという点です。 — location: [220](kindle://book?action=open&asin=B07X1WL8XN&location=220) ^ref-4435
>>[!note]
>なるほど、そこも大切なんですな。
---
>[!quote]
>記号主義は、人間の思考の対象はすべて記号化することができ（物理記号システム仮説）、その記号を論理的に操作することで知能を再現できるという立場です。 — location: [243](kindle://book?action=open&asin=B07X1WL8XN&location=243) ^ref-61118
>>[!note]
>面白い。そらそうか。
---
>[!quote]
>コンピュータに「りんごは赤い」「りんごは甘い」といった知識をインプットしたとしても、「赤い」「甘い」が指し示す実体験を理解させることは不可能ということです。これを、シンボルグラウンディング問題といいます。 — location: [247](kindle://book?action=open&asin=B07X1WL8XN&location=247) ^ref-6057
>>[!note]
>シンボルグラウンディング問題、カッコいいね。
---
>[!quote]
>第2次人工知能ブームの終焉以降、 ソフトコンピューティング と呼ばれる生命の柔軟性を模倣した計算方法が注目を集めるようになります。ニューラルネットワーク、ファジィ理論、遺伝的アルゴリズム、強化学習などがその代表例です。同時に、統計学を応用した機械学習（回帰分析など）の手法も地道に発展を続けていました。 — location: [289](kindle://book?action=open&asin=B07X1WL8XN&location=289) ^ref-41494
>>[!note]
>やっぱりこういう「機械学習」と「深層学習」の棲み分けがよくわからないんだよね。浅い一般的な例え話と、正確な違いを把握しないとね。
---
>[!quote]
>教師あり学習(supervised learning) とは、正解となる答えが含まれたデータをモデルに学習させる方法のことです。ここでのモデルは、人工知能の脳にあたる部分と考えてかまいません。また、正解となる答えを ラベル といい、答えが含まれたデータを ラベル付きデータ（もしくは 訓練データ）と呼びます。教師あり学習はモデルの学習にラベル付きデータを用いますが、最終的な目標はラベルのないデータ（テストデータ）を正解させることです。 — location: [325](kindle://book?action=open&asin=B07X1WL8XN&location=325) ^ref-11407
>>[!note]
>こういう用語込みの定義は一番抑えなきゃいけないところだね。
---
>[!quote]
>Section02では、分類を「データ全体をできるだけ分けるように線を引くこと」、回帰を「回帰はデータ全体にできるだけ重なるように線を引くこと」と解説しました。 — location: [337](kindle://book?action=open&asin=B07X1WL8XN&location=337) ^ref-61716
>>[!note]
>浅く、わかりやすい例であり本質。
---
>[!quote]
>一方の回帰は、答えが連続した数値（ 連続値）になります。株価の予測問題を考えてみると、答えが12345.6円のような中途半端な値になっても意味が通ります。そのため、株価の予測問題は回帰に分類されます。 — location: [347](kindle://book?action=open&asin=B07X1WL8XN&location=347) ^ref-23391

---
>[!quote]
>人間はいくつかのものを見るとき、意識しないままそれぞれの特徴をもとに「区別」しようとします。たとえばあなたが、図のように並べられた野菜や果物を見たとき、たとえ名前を知らなかったとしてもただ漠然と眺めることはしないはずです。色で分けてみたり形で分けてみたりし、どんなグループを作ればその状況をうまく説明することができるのかを考えるでしょう。そして野菜や果物を色で区別することによって初めて、「ここには6色の野菜や果物があります」といった風にまとめることができます。教師なし学習ではこの人間の「特徴をとらえる」能力をアルゴリズムにより再現することを目指しているのです。 — location: [387](kindle://book?action=open&asin=B07X1WL8XN&location=387) ^ref-30570
>>[!note]
>本質。だいぶ本質。
---
>[!quote]
>次元削減の一例としては、データの可視化が挙げられます。私たちが多次元データを直感的に理解するためには、データの次元を人間が視認することができる3次元以下に落とした上で可視化しなくてはなりません。 — location: [410](kindle://book?action=open&asin=B07X1WL8XN&location=410) ^ref-59158
>>[!note]
>急に次元の話出てきてアツいと思ったけど、山田さんは確かに三三次元の話してた気がしてきたわ。
---
>[!quote]
>統計は「 なぜこのようなデータが出るのか」を教えてくれます。一方、機械学習は「 これからデータがどう変わっていくのか」を、それぞれ教えてくれる — location: [444](kindle://book?action=open&asin=B07X1WL8XN&location=444) ^ref-22958
>>[!note]
>大きい違い。俺の中では「理由の後付け」であるアートと「理屈から組み立てる」デザインの違いと捉えている問題。
---
>[!quote]
>統計は今あるモデルを使ってうまく「データを説明する」ための分野 と言い換えることもできるでしょう。 一方、機械学習は「 データを予測する」ことにフォーカスした分野です。 — location: [461](kindle://book?action=open&asin=B07X1WL8XN&location=461) ^ref-39015
>>[!note]
>この言い回しも含めて覚えておきましょう。
---
>[!quote]
>機械が知能を持つとは、ものごとを「分ける」ことができるようになることといえます。たとえば今日はアイスが売れるのか、売れないのか。ある物体がリンゴなのか、そうではないのか。ある事業で利益が出るのか、出ないのか。 Section04で学んだように、人工知能はパターン探索や知識の蓄積でこの「わける」機能を実現しようとしてきましたが、どちらもパターンが多くなるなどの問題でうまくいかなくなってしまいました。 機械学習とは、この「分ける」能力の実現のために、今までの「演繹的思考」ではなく「統計的思考」という新たなアプローチを取り入れることで、知能を実現しようとする人工知能です。 — location: [493](kindle://book?action=open&asin=B07X1WL8XN&location=493) ^ref-27519
>>[!note]
>機械学習に統計の知識が必要な理由です。重要ポイント。
---
>[!quote]
>ものを見分けるとき、人間は見た目や匂い、感触などから情報を入手して見分けます。対して機械学習は、情報を果物の色の濃淡や重さ、匂い成分の量などの｢ 特徴量｣という数値で取り込みます。この特徴量を決めるのは人間の仕事であり、このことを 特徴量設計 といいますが、実はこの特徴量の決め方でアルゴリズムの性能は大きく変わってしまうのです。たとえばリンゴとナシを見分ける特徴量として、「赤さ」や「甘さ」などはよさそうですが、「丸さ」や「表面のなめらかさ」はあまり差がなさそうだと分かります。 — location: [506](kindle://book?action=open&asin=B07X1WL8XN&location=506) ^ref-4015
>>[!note]
>曖昧な感覚値をどのように具体的な数値に落とし込むか、という話ですね。
---
>[!quote]
>機械学習は、学習データを入力したときの出力が正答に近い値になるよう、モデルを自動で最適化（学習）するアルゴリズムです（教師あり学習）。つまり、必ずしも人間の思考のように推論が進むとは限らず、その過程を見ても根拠がわからないことが多いのです。そのため機械学習で病気を診断したとして、「あなたは〇〇という病気である可能性が高いです。しかし根拠はわかりません」という結論が出るかもしれません。これでは当然、患者を納得させることもできないでしょう。このように、根拠が重要となる推論が必要な分野において、機械学習のみで結論を出すことは難しいのです。 — location: [566](kindle://book?action=open&asin=B07X1WL8XN&location=566) ^ref-17414
>>[!note]
>この分野分けのポイントは大事にしなきゃいけなさそうだね。
---
>[!quote]
>外部からデータを取得する場合と異なり自分でデータを記録する場合には、「データの蓄積にかかる期間」も考慮に入れる必要があります。例として、ある顧客がサービスを解約する可能性を機械学習で予測することを考えてみましょう。年間数件しか解約が発生しない場合には、5年間収集したとしてもせいぜい数十件程度しか集まりません。 — location: [693](kindle://book?action=open&asin=B07X1WL8XN&location=693) ^ref-6926
>>[!note]
>大事な部分かもしれない。ビッグデータと呼ばれる分野ならなおさら。
---
>[!quote]
>Web APIは、一般的にはWebサービスの提供者が用意したアプリケーションです。Web APIを利用することで、そのWebサービスで扱われているデータを取得できます。取得できるデータはWebサービスによってさまざまです。たとえばFacebookやTwitterなどのSNSのWeb APIでは、SNS内の投稿やユーザー情報、トレンドなどの情報を、また大手インターネット通販サービスのAmazonのWeb APIでは商品情報や売れ筋などの情報を取得できます。 — location: [721](kindle://book?action=open&asin=B07X1WL8XN&location=721) ^ref-15508
>>[!note]
>松尾さんがこの辺りめちゃくちゃ勝手に使っていると同時に、やれることは本当に多そうなんだよな。「Webの分野だからやらない」という思考よりも「その中でもどの辺なら活かせそうなのか」の思考でやっていく方が良さそうだと感じるぞ。
---
>[!quote]
>機械学習における モデル とは、入力されたデータからある出力（入力データに対する分類や予測）を導き出すための数理モデルのことです。このことは、「 何かを入れると何かが出てくる箱」をイメージすると、わかりやすくなります。箱はそれぞれ大きさや入口の形が異なっており、それによってどんなものを入れられるかが決まります。同様に、箱から出てくるものの形も決まっていると考えてください。そのようなモデルを作成するためには、入出力するデータがどのようなものであるかを最初に決定しなければなりません。 ちなみにこの「箱」は、数学における「 関数」にあたります。機械学習において、アルゴリズムの中で行われていることは関数の計算なのです。 — location: [786](kindle://book?action=open&asin=B07X1WL8XN&location=786) ^ref-17879
>>[!note]
>ソニーの応募条件で求められているの、絶対にここだよな。少しイメージが湧いてきたかも。
---
>[!quote]
>オンライン学習の欠点は、異常なデータが入力されるとモデルの予測能力が低くなることです。これは、新しく与えられたデータは例外なく正しい分類としてパラメータを更新するためです。これを防ぐには、異常検出アルゴリズムを使うなどして異常なデータの入力を監視する必要があります。 — location: [862](kindle://book?action=open&asin=B07X1WL8XN&location=862) ^ref-39060
>>[!note]
>こういった修正方法の特徴はしっかり覚える必要があるね。
---
>[!quote]
>汎化性能を検証するうえで大事なことは、「学習に利用したデータを検証に使わない」ということです。そのためには、学習データとは別に検証するためのデータを分けて残しておく必要があります。この検証用のデータを、 テスト（検証・評価）データ と呼びます。 学習データとテストデータをわけることで、学習したモデルが未知のデータに対してどのくらいの性能があるのかを検証することができ、そこで初めてそのモデルの汎化性能を評価できます。作成したモデルで期待される性能がわからないことは、モデルの信頼性を保証できないということであり、アルゴリズムを実際に活用する上で大きな支障となるのです。 — location: [884](kindle://book?action=open&asin=B07X1WL8XN&location=884) ^ref-6896
>>[!note]
>言われてみれば「それはそうか」の気持ち。
---
>[!quote]
>前のセクションでは、テストデータによって機械学習モデルの性能を検証する方法を学びました。検証においてモデルにテストデータを入力すると、それぞれのデータに対し、回帰であれば何らかの数値が、分類であればデータのラベルが予測されます。それらの結果は、回帰であればそれぞれの入力において予測と正答がどれぐらい離れているのかが値として現れます。一方の分類であれば、それぞれの入力がそれぞれ何のラベルに分類されたのか、表として集計できます。しかしこのような集計だけでは、「このモデルはどんな性能なのか」、あるいは「このモデルによる事業への利益は見込めるのか」といった重要な疑問に答えられません。 — location: [927](kindle://book?action=open&asin=B07X1WL8XN&location=927) ^ref-42737
>>[!note]
>当たり前のように「回帰と分類」で話を進めているけど、回帰が何かをここにきて正しく理解していない。
---
>[!quote]
>2ラベル分類の混合行列の4パターンはそれぞれ、正解が「〇」であるものを正しく「〇」と予想した回数である TP（True Positive：真陽性）、正解が「〇」であるものを間違って「×」と予想した回数である FN（False Negative：偽陰性）、正解が「×」であるものを間違って「〇」と予想した回数である FP（False Positive：偽陽性）、正解が「×」であるものを正しく「×」と予想した回数である TN（True Negative：真陰性） と呼ばれます。 — location: [961](kindle://book?action=open&asin=B07X1WL8XN&location=961) ^ref-36822
>>[!note]
>この表記の仕方自体がまず愉快だね。覚えたいのもそうだけど、これがどう作用するかをまた見たい。
---
>[!quote]
>機械学習（特に教師あり学習）を行うためには大量のラベル（正解）付きデータが必要ですが、ラベル付けの作業は煩雑です。そのため、教師データをやみくもに作って学習（受動学習）するのではなく、教師データの数を絞って学習する 能動学習 を採用すると効率的です。 — location: [1053](kindle://book?action=open&asin=B07X1WL8XN&location=1053) ^ref-51241
>>[!note]
>まずはここが能動学習の肝。
---
>[!quote]
>統計学（や機械学習）では相関関係を分析することはできますが、相関関係だけで因果関係を結論づけるには無理があります。 — location: [1105](kindle://book?action=open&asin=B07X1WL8XN&location=1105) ^ref-23617
>>[!note]
>冷静に考えたらそう。言葉の意味合いをしっかり掴むこと。
---
>[!quote]
>データから因果分析を行う際には、下表のようなガイドラインがあります。データを分析した結果、原因とされるもの（“原因”）と結果とされるもの（“結果”）が因果関係にあると判断するためのものです。このガイドラインは、主に生物学・医学の研究で使われているものですが、機械学習や統計を利用したデータ分析に役立つ部分も多いでしょう。 — location: [1119](kindle://book?action=open&asin=B07X1WL8XN&location=1119) ^ref-16454
>>[!note]
>「このうちのどれに当てはまるか」みたいなものを判別できるようにならなきゃいけないね。
---
>[!quote]
>機械学習ではモデルの中身がブラックボックスとなるため、振る舞いを監視することが重要です。 — location: [1158](kindle://book?action=open&asin=B07X1WL8XN&location=1158) ^ref-8200
>>[!note]
>原因というか、中での判別状況を人が見て弄って治す、みたいなレベルじゃわからないんだね。
---
>[!quote]
>フィードバックループ とは、システムの振る舞いが環境に影響を及ぼし、次に観測するデータが環境から影響を受けて変化してしまう現象です。 この際、システムの振る舞いの変化が急であったり頻繁に起こったりする場合は、振る舞いの変化の検出は比較的かんたんです。一方、システムの振る舞いが徐々に変わっていったり、モデルの更新の頻度が低い場合には、振る舞いの変化に気づくのが遅れる場合があります。 — location: [1163](kindle://book?action=open&asin=B07X1WL8XN&location=1163) ^ref-40197
>>[!note]
>怖いねぇ…気付くのが遅れるのが一番厄介。
---
>[!quote]
>回帰とは、「データにもっともフィットする線を引くこと」だと思ってよいでしょう。 — location: [1189](kindle://book?action=open&asin=B07X1WL8XN&location=1189) ^ref-13978
>>[!note]
>定義助かる。
---
>[!quote]
>単純な最小二乗法では、誤差関数を誤差の二乗和として誤差関数を最小化していました。説明変数の選び方が適切でないときに単純な最小二乗法を行うとどんどん回帰係数が大きくなってしまいますが、罰則項によってブレーキをかけることができます。 — location: [1236](kindle://book?action=open&asin=B07X1WL8XN&location=1236) ^ref-10457

---
>[!quote]
>線形回帰においては、L1正則化を用いる回帰を ラッソ回帰、L2正則化を用いる回帰を リッジ回帰 と呼びます。さらに、L1, L2正則化の両方を用いたものは Elastic Net回帰 と呼ばれています。 — location: [1249](kindle://book?action=open&asin=B07X1WL8XN&location=1249) ^ref-34993
>>[!note]
>線形回帰。線形じゃない回帰ってなに？線形も分からんし。
---
>[!quote]
>Section02で、2店舗の派閥分類を例に、データの点をもっとも引き離すような境界線を引くことが分類だと解説しましたが、あれこそがSVMの考え方でもあるのです。 — location: [1266](kindle://book?action=open&asin=B07X1WL8XN&location=1266) ^ref-60755

---
>[!quote]
>サポートベクター（ベクトル） とは、境界にもっとも近いデータの点のことです。新しいデータが入力されたときの誤判定を防ぐため、境界に近いデータであっても境界からできるだけ離すことが重要になります。 — location: [1269](kindle://book?action=open&asin=B07X1WL8XN&location=1269) ^ref-59513

---
>[!quote]
>境界は、特徴量の数が2つなら2次元平面上の直線として、3つなら3次元空間上の平面として表されます。 なお、4つ以上の場合、4次元(以上の)空間を考えなければならず、正確な図示は不可能です。そのような4次元以上の空間における分類境界は、 超平面 と呼ばれます。 — location: [1274](kindle://book?action=open&asin=B07X1WL8XN&location=1274) ^ref-13263
>>[!note]
>超平面は初耳単語です。
---
>[!quote]
>SVMでは、直線・平面・超平面を境界としてデータを分離しますが、このことを 線形分離 といいます。しかし実際のデータにおいては、線形分離が可能なケースというのは多くないのです。 — location: [1278](kindle://book?action=open&asin=B07X1WL8XN&location=1278) ^ref-11118
>>[!note]
>ここでも線形。でも少しイメージ湧くからよし。
---
>[!quote]
>カーネル法は、派閥の境界がそもそも曲がった形（非線形）になっている場合に用います。派閥の境界が曲がっていた場合は、データから新しい特徴量を作って、うまく線形分離可能になるようにプロットしなければなりません。この作業を「 線形分離可能な高次元特徴空間に写像する」と表現します。 — location: [1290](kindle://book?action=open&asin=B07X1WL8XN&location=1290) ^ref-5104
>>[!note]
>線じゃない形、曲線的な場合が非線形。
---
>[!quote]
>実際のデータでは、データに存在する特徴量から、うまく線形分離可能になるような特徴量を新たに作り出さなくてはなりません。その作り方を指定するのがカーネル関数です。カーネル関数としてはガウシアン（RBF）カーネルや多項式カーネルなどがあります。カーネル関数を使いつつ計算量を減らす工夫は、カーネルトリックと呼ばれます。 — location: [1297](kindle://book?action=open&asin=B07X1WL8XN&location=1297) ^ref-37051
>>[!note]
>ガウシアンカーネル、カーネルトリック、うん、カッコイイ。
---
>[!quote]
>ここまで取り上げてきたSVMの長所は主に4つです。①特徴量が多い場合に有効であること、②特徴量の数がデータの数より多い場合でも有効であること、③境界となる直線・平面・超平面（二次元の平面をそれ以外の次元へ一般化すること）を引く際には、境界に近い点のみを考慮すればよいため、データが多い場合もメモリを節約できること、④さまざまなカーネル関数を使うことができるため、多様な出力結果を得られることが挙げられます。 — location: [1301](kindle://book?action=open&asin=B07X1WL8XN&location=1301) ^ref-24655

---
>[!quote]
>SVMは分類のほか回帰に応用される場合があります。SVMを利用した回帰を サポートベクトル回帰（SVR） と — location: [1308](kindle://book?action=open&asin=B07X1WL8XN&location=1308) ^ref-26709

---
>[!quote]
>決定木 において、条件となる部分を ノード（節点） といい、一番上の条件の部分を根ノード、決定木の分類を示している末節の部分を葉ノードといいます。決定木の一部で、それ自体も木になっているものを部分木といいます。 — location: [1319](kindle://book?action=open&asin=B07X1WL8XN&location=1319) ^ref-29461
>>[!note]
>基礎の基礎、しっかり固めて覚えて。
---
>[!quote]
>剪定 は、決定木の過学習防止に効果的な方法です。多く使われる事後剪定(post-pruning)では、訓練データを使って決定木を意図的に過学習させたのち、検証データを使って性能の悪い決定木の分岐を切り取ります。 — location: [1348](kindle://book?action=open&asin=B07X1WL8XN&location=1348) ^ref-23768
>>[!note]
>こういう作業が何よりも多くて大事よねおそらく。
---
>[!quote]
>アンサンブル学習 では、高精度のモデルを1つ作るのではありません。精度の低いモデルをたくさん作って合体させることで、高精度のモデルを作るのが目標です。 — location: [1360](kindle://book?action=open&asin=B07X1WL8XN&location=1360) ^ref-9493

---
>[!quote]
>ロジスティック回帰 は教師あり学習の一種で、主に分類に利用されるアルゴリズムです。ロジスティック回帰を利用することで、ある顧客が商品を買うかどうかといった「Yes / No」を確率として計算し推測することができます。 — location: [1446](kindle://book?action=open&asin=B07X1WL8XN&location=1446) ^ref-9414

---
>[!quote]
>しかし、「一番ふさわしい」値だけを求めることは「その値がどれだけふさわしいのか」、「他の値はどれだけふさわしいのか」といった情報を捨てていることも意味します。単回帰は直線を引くことであるため、データが2個あれば「一番ふさわしい」○と△の値を求められます。データが少ないほど分析は信頼できなくなる（ふさわしくなくなる）ことは直感でわかりますが、最尤推定ではデータが2個のときも1000個のときも○と△の値が出力されるだけで、「その値がどれだけふさわしいのか」がわからないのです。 — location: [1481](kindle://book?action=open&asin=B07X1WL8XN&location=1481) ^ref-2970
>>[!note]
>言われてみればというか、それはそうだけど、というか…。
---
>[!quote]
>推定では、推定結果を「値」と「その値が推定結果である確率」のペア（分布と呼びます）で表します。これにより、値がどれだけふさわしいのかがわかります。さらに、「値がどれくらいになりそうか」という予想（ 事前分布）をあらかじめ立てておき、新しいデータを見て、事前分布を修正するという手続き（ ベイズ更新）を踏みます。修正後の分布は 事後分布 といいます。予想はデータに基づかない主観的なものでもよいため、推定にデータ以外の知識を反映できるようになります。 — location: [1486](kindle://book?action=open&asin=B07X1WL8XN&location=1486) ^ref-56965
>>[!note]
>ちょうど統計Webで学んでいるところです（7月初旬現在）
---
>[!quote]
>①の大きな特徴は、これらのアルゴリズムが「何らかのデータに特化して設計されているのではない」ということです。あくまでデータを学習させて予測結果を求めることに関心があるため、「データがどのように発生しているのか」について考える必要がありません。このアプローチを使うと、高度な数学の知識がなくてもかんたんなプログラムだけでデータの学習と予測を行うことができます。 — location: [1506](kindle://book?action=open&asin=B07X1WL8XN&location=1506) ^ref-7691

---
>[!quote]
>ベイジアンモデルは②に分類されます。このアプローチでは「データがどのように発生しているのか」というデータの発生構造（モデル）の候補をあらかじめ設計しておき、データを使ってそのモデルを推定します。そして、推定したモデルをもとに予測を行うのです。②では、対象とするデータに応じて考えるべきモデルを拡張したり組み合わせたりします。つまりデータの予測結果だけではなく、モデルにも関心があるわけです。このアプローチでは目的に合ったモデルを考えていくため、①よりも原理的に達成される性能は高くなります。 — location: [1510](kindle://book?action=open&asin=B07X1WL8XN&location=1510) ^ref-22572
>>[!note]
>アルゴリズム的な結果と予測だけを見るアプローチと、数学(統計？)的な要因を探るアプローチの違い、という感覚。
---
>[!quote]
>どのようなモデルを設計するかがもっとも重要なのですが、②の工程は計算が複雑になるため面倒です。そこで、①モデルの設計に集中するための 確率的プログラミング言語 が用意されています。これを使うと、モデルを設計してデータを用意するだけで、データの学習からモデルの推定・予測までを行ってくれるようになります。 — location: [1526](kindle://book?action=open&asin=B07X1WL8XN&location=1526) ^ref-26377
>>[!note]
>非常に便利。人間の頭脳なんて頼らないのが一番なんだから。
---
>[!quote]
>次元削減 はその名の通り、データの次元数を減らす処理のことを指します。ここでいうデータの次元とは、たとえば学生の成績データがあった場合の国語の点数、数学の点数、英語の点数……といったデータの項目数のことです。 — location: [1640](kindle://book?action=open&asin=B07X1WL8XN&location=1640) ^ref-36150

---
>[!quote]
>次元削減ではデータ量を削減しているため、当然データの本来持っていた情報の一部は失われています。この失われる情報の量のことを 情報損失量 といいます。 — location: [1683](kindle://book?action=open&asin=B07X1WL8XN&location=1683) ^ref-21325

---
>[!quote]
>ある関数（目的関数）の値が最大または最小になるような解を探すことを最適化とよびます。たとえば下図の赤線のような関数の値を最大にするとき、関数の形がこのように見えればどこが最大になる解であるかが一目瞭然ですが、実際には多くの場合、目的関数の形はわかりません。そこで最適化では、試しにいくつか解を入力することによって、より目的関数の値がよい解を探し、最適解を求めるのです。 — location: [1698](kindle://book?action=open&asin=B07X1WL8XN&location=1698) ^ref-51524

---
