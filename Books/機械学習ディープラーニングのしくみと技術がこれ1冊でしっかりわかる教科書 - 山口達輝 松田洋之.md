---
tags:
  - Machine_Learning
  - 📚Book/Techs
title: 429710640X 9784297106409
aliases:
  - 機械学習&ディープラーニングのしくみと技術がこれ1冊でしっかりわかる教科書-山口達輝, 松田洋之
author:
  - 山口達輝
  - 松田洋之
publisher: 
publish: 2019-09
total: 240
isbn: 429710640X 9784297106409
cover: http://books.google.com/books/content?id=ShRPyQEACAAJ&printsec=frontcover&img=1&zoom=1&source=gbs_api
created: 2025-08-10 12:38:11
updated: 2025-08-10 12:38:11
source: Kindle
---
![cover|150](http://books.google.com/books/content?id=ShRPyQEACAAJ&printsec=frontcover&img=1&zoom=1&source=gbs_api)
# 機械学習&ディープラーニングのしくみと技術がこれ1冊でしっかりわかる教科書
author: [山口達輝, 松田洋之]
publisher: 
publish: 2019-09
Started: 2025-06-23
Finished: 2025-08-11
## 要約
## 読書メモ
分類はデータがどの種類に属すかを見ること、回帰は分類とは反対にデータの傾向を見ること。
- カテゴリデータの整形には「ラベルエンコーディング」、「カウントエンコーディング」、「One-Hot エンコーディング」がある。
- 数値データの整形には「離散化」、「対数変換」、「スケーリング」といったテクニックがある。
- 未知のデータに対する予測や分類の精度である汎化性能を検証方法には「ホールドアウト検証」や「K-分割交差検証（K-foldクロスバリデーション）」、「Leave -one-out交差」がある。
- 機械学習モデルの性能はテストデータを利用した検証結果から評価する。回帰モデルの性能の評価は「予測誤差」によって行い、その集計方法として「$R^2$（決定係数）」、「RMSE（平方平均二乗誤差）」、「MAE（平均絶対誤差）」がある。
- 分類モデルを評価する場合、「True」or「False」と「Positive」or「Negative」の混合行列の表を用いて評価を行う。その評価指数としては「正解率（Accuracy）」、「再現率（Recall）」、「適合率（Precision）」、「F値（f-score）」を用いる。
- ハイパーパラメータがおかしいと、未学習による「近似誤差」や過学習による「推定誤差」が起こる。
- ハイパーパラメータの組み合わせを決める手法には、グリッドサーチの他に「ランダムサーチ」、「焼きなまし法」、「ベイズ最適化」、「遺伝的アルゴリズム」などが存在する。
- 機械学習に必要なラベル付きデータの能動学習を行うために、紛らわしいラベル付きデータが必要となる。その作成手法としては、「Membership Query Synthesis」や「Stream-Based Selective Sampling」、「Pool-Based Sampling」といったものがある。
- 相関関係から因果関係を見抜く方法として、「ランダム化比較試験（A/Bテストとも呼ばれる）」がある。実験ができない場合は擬似実験を行って見抜く。その分析手法の一つに「回帰分断デザイン」がある。
- 機械学習のアルゴリズムを考えるときに、「回帰分析の方法」の種類と、「回帰時の誤差を少なくする方法」をセットで想定する必要がある。
	- 単回帰と最小二乗法
	- 重回帰と多重共線性
	- 多項式回帰
	- ロバスト回帰とRANSAC（Random Sample）、Theil-Sen推定量（Theil-Sen estimator）やHuber損失
- 活性化関数として代表的なものに「シグモイド関数」、「ハイパボリックタンジェント（tanh）関数」、「Rectified Linear Unit（ReLU）関数」などが挙げられる。
- ニューラルネットワークの学習には「順伝播」の流れと逆に伝播するよう計算を行い、重みを調整する「誤差逆伝播法」が代表的手法の一つとして挙げられる。
- ニューラルネットワークのモデルの学習において最適化される目的関数は、「誤差逆伝播法」で計算される「損失関数」に当たる。
- ニューラルネットワークのモデル最適化でもっとも利用されているのは「勾配降下法」と呼ばれるものである。
## ハイライト/メモ
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 02 機械学習（ML）とは > ページ20 ·位置177
</div>
<div class="noteText">
    機械学習の問題は、大きく分類と回帰に分けられます。
</div><div class="noteHeading">
    メモ - 02 機械学習（ML）とは > ページ20 ·位置178
</div>
<div class="noteText">
    ここスーパー基礎、大事ポイント。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 02 機械学習（ML）とは > ページ20 ·位置182
</div>
<div class="noteText">
    グラフ上にデータがプロット（書き込むこと）されているとすれば、分類はデータ全体をできるだけ分けるように線を引くこと、回帰はデータ全体にできるだけ重なるように線を引くことに相当します。
</div><div class="noteHeading">
    メモ - 02 機械学習（ML）とは > ページ20 ·位置184
</div>
<div class="noteText">
    概念を掴む。この違いを捉えて忘れないで。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 03 ディープラーニング（DL）とは > ページ25 ·位置218
</div>
<div class="noteText">
    隠れ層とは「入力層から受け取った情報をさまざまな組み合わせで伝えていき、出力層に役立つ形に情報を変形して渡す」という役割を持つ層のことです。
</div><div class="noteHeading">
    メモ - 03 ディープラーニング（DL）とは > ページ25 ·位置220
</div>
<div class="noteText">
    ディープラーニングの特徴とのことだが、いまいち理解できていない。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 03 ディープラーニング（DL）とは > ページ25 ·位置220
</div>
<div class="noteText">
    ディープラーニングが画期的だったのは、最適な特徴量を自動的に抽出するという点です。
</div><div class="noteHeading">
    メモ - 03 ディープラーニング（DL）とは > ページ25 ·位置221
</div>
<div class="noteText">
    なるほど、そこも大切なんですな。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 03 ディープラーニング（DL）とは > ページ29 ·位置243
</div>
<div class="noteText">
    記号主義は、人間の思考の対象はすべて記号化することができ（物理記号システム仮説）、その記号を論理的に操作することで知能を再現できるという立場です。
</div><div class="noteHeading">
    メモ - 03 ディープラーニング（DL）とは > ページ29 ·位置244
</div>
<div class="noteText">
    面白い。そらそうか。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 03 ディープラーニング（DL）とは > ページ29 ·位置247
</div>
<div class="noteText">
    コンピュータに「りんごは赤い」「りんごは甘い」といった知識をインプットしたとしても、「赤い」「甘い」が指し示す実体験を理解させることは不可能ということです。これを、シンボルグラウンディング問題といいます。
</div><div class="noteHeading">
    メモ - 03 ディープラーニング（DL）とは > ページ29 ·位置249
</div>
<div class="noteText">
    シンボルグラウンディング問題、カッコいいね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 04 人工知能と機械学習が普及するまで > ページ35 ·位置289
</div>
<div class="noteText">
    第 2次人工知能ブームの終焉以降、ソフトコンピューティングと呼ばれる生命の柔軟性を模倣した計算方法が注目を集めるようになります。ニューラルネットワーク、ファジィ理論、遺伝的アルゴリズム、強化学習などがその代表例です。同時に、統計学を応用した機械学習（回帰分析など）の手法も地道に発展を続けていました。
</div><div class="noteHeading">
    メモ - 04 人工知能と機械学習が普及するまで > ページ36 ·位置293
</div>
<div class="noteText">
    やっぱりこういう「機械学習」と「深層学習」の棲み分けがよくわからないんだよね。浅い一般的な例え話と、正確な違いを把握しないとね。
</div><div class="sectionHeading">
    2章 機械学習の基礎知識
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 05 教師あり学習のしくみ > ページ41 ·位置325
</div>
<div class="noteText">
    教師あり学習( supervised learning)とは、正解となる答えが含まれたデータをモデルに学習させる方法のことです。ここでのモデルは、人工知能の脳にあたる部分と考えてかまいません。また、正解となる答えをラベルといい、答えが含まれたデータをラベル付きデータ（もしくは訓練データ）と呼びます。教師あり学習はモデルの学習にラベル付きデータを用いますが、最終的な目標はラベルのないデータ（テストデータ）を正解させることです。
</div><div class="noteHeading">
    メモ - 05 教師あり学習のしくみ > ページ41 ·位置330
</div>
<div class="noteText">
    こういう用語込みの定義は一番抑えなきゃいけないところだね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 05 教師あり学習のしくみ > ページ42 ·位置337
</div>
<div class="noteText">
    Section 02では、分類を「データ全体をできるだけ分けるように線を引くこと」、回帰を「回帰はデータ全体にできるだけ重なるように線を引くこと」と解説しました。
</div><div class="noteHeading">
    メモ - 05 教師あり学習のしくみ > ページ42 ·位置339
</div>
<div class="noteText">
    浅く、わかりやすい例であり本質。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 05 教師あり学習のしくみ > ページ43 ·位置347
</div>
<div class="noteText">
    一方の回帰は、答えが連続した数値（連続値）になります。株価の予測問題を考えてみると、答えが 12345. 6円のような中途半端な値になっても意味が通ります。そのため、株価の予測問題は回帰に分類されます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 06 教師なし学習のしくみ > ページ49 ·位置387
</div>
<div class="noteText">
    人間はいくつかのものを見るとき、意識しないままそれぞれの特徴をもとに「区別」しようとします。たとえばあなたが、図のように並べられた野菜や果物を見たとき、たとえ名前を知らなかったとしてもただ漠然と眺めることはしないはずです。色で分けてみたり形で分けてみたりし、どんなグループを作ればその状況をうまく説明することができるのかを考えるでしょう。そして野菜や果物を色で区別することによって初めて、「ここには 6色の野菜や果物があります」といった風にまとめることができます。教師なし学習ではこの人間の「特徴をとらえる」能力をアルゴリズムにより再現することを目指しているのです。
</div><div class="noteHeading">
    メモ - 06 教師なし学習のしくみ > ページ49 ·位置392
</div>
<div class="noteText">
    本質。だいぶ本質。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 06 教師なし学習のしくみ > ページ50 ·位置399
</div>
<div class="noteText">
    階層的クラスタリングとは、特徴の似ているクラスタ同士を 1つずつ結合させていき、最終的に 1つの大きなクラスタになるまでくり返すことでクラスタリングを行う手法です。対して非階層的クラスタリングとは、初めにクラスタ数（下図では 3つ）を設定し、そのクラスタ数でもっともよくデータを分けることができるようクラスタリングを行う手法です。なお本書では、 Section 31にて非階層クラスタリングの代表的なアルゴリズムである「 k平均( k-means)法」を紹介しています。
</div><div class="noteHeading">
    メモ - 06 教師なし学習のしくみ > ページ51 ·位置404
</div>
<div class="noteText">
    「アルゴリズム図鑑」の本で読んだところ。だいぶスッキリしました。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 06 教師なし学習のしくみ > ページ52 ·位置410
</div>
<div class="noteText">
    次元削減の一例としては、データの可視化が挙げられます。私たちが多次元データを直感的に理解するためには、データの次元を人間が視認することができる 3次元以下に落とした上で可視化しなくてはなりません。
</div><div class="noteHeading">
    メモ - 06 教師なし学習のしくみ > ページ52 ·位置411
</div>
<div class="noteText">
    急に次元の話出てきてアツいと思ったけど、山田さんは確かに三三次元の話してた気がしてきたわ。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 07 強化学習のしくみ > ページ54 ·位置427
</div>
<div class="noteText">
    強化学習とは、赤ちゃんが一人で立ち上がれるようになるのと同じように、正解を与えなくても試行錯誤をくり返しながら最適な行動をするように学習する方法のことです。教師あり学習には明示的な正解がありましたが、強化学習にはありません。かわりに、その行動がどれだけよかったのかを報酬として与え、その報酬が高くなるような行動をするように仕向けるのです。教師なし学習も正解はありませんが、強化学習とは性質が全く異なります。
</div><div class="noteHeading">
    メモ - 07 強化学習のしくみ > ページ54 ·位置431
</div>
<div class="noteText">
    教師あり学習、教師なし学習と強化学習の違いは正しく抑えよう。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 08 統計と機械学習の違い > ページ58 ·位置445
</div>
<div class="noteText">
    統計は「なぜこのようなデータが出るのか」を教えてくれます。一方、機械学習は「これからデータがどう変わっていくのか」を、それぞれ教えてくれる
</div><div class="noteHeading">
    メモ - 08 統計と機械学習の違い > ページ58 ·位置447
</div>
<div class="noteText">
    大きい違い。俺の中では「理由の後付け」であるアートと「理屈から組み立てる」デザインの違いと捉えている問題。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 08 統計と機械学習の違い > ページ60 ·位置461
</div>
<div class="noteText">
    統計は今あるモデルを使ってうまく「データを説明する」ための分野と言い換えることもできるでしょう。一方、機械学習は「データを予測する」ことにフォーカスした分野です。
</div><div class="noteHeading">
    メモ - 08 統計と機械学習の違い > ページ60 ·位置464
</div>
<div class="noteText">
    この言い回しも含めて覚えておきましょう。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 09 機械学習と特徴量 > ページ65 ·位置493
</div>
<div class="noteText">
    機械が知能を持つとは、ものごとを「分ける」ことができるようになることといえます。たとえば今日はアイスが売れるのか、売れないのか。ある物体がリンゴなのか、そうではないのか。ある事業で利益が出るのか、出ないのか。 Section 04で学んだように、人工知能はパターン探索や知識の蓄積でこの「わける」機能を実現しようとしてきましたが、どちらもパターンが多くなるなどの問題でうまくいかなくなってしまいました。機械学習とは、この「分ける」能力の実現のために、今までの「演繹的思考」ではなく「統計的思考」という新たなアプローチを取り入れることで、知能を実現しようとする人工知能です。
</div><div class="noteHeading">
    メモ - 09 機械学習と特徴量 > ページ66 ·位置499
</div>
<div class="noteText">
    機械学習に統計の知識が必要な理由です。重要ポイント。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 09 機械学習と特徴量 > ページ66 ·位置506
</div>
<div class="noteText">
    ものを見分けるとき、人間は見た目や匂い、感触などから情報を入手して見分けます。対して機械学習は、情報を果物の色の濃淡や重さ、匂い成分の量などの｢特徴量｣という数値で取り込みます。この特徴量を決めるのは人間の仕事であり、このことを特徴量設計といいますが、実はこの特徴量の決め方でアルゴリズムの性能は大きく変わってしまうのです。たとえばリンゴとナシを見分ける特徴量として、「赤さ」や「甘さ」などはよさそうですが、「丸さ」や「表面のなめらかさ」はあまり差がなさそうだと分かります。
</div><div class="noteHeading">
    メモ - 09 機械学習と特徴量 > ページ67 ·位置512
</div>
<div class="noteText">
    曖昧な感覚値をどのように具体的な数値に落とし込むか、という話ですね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 10 得意な分野、苦手な分野 > ページ74 ·位置548
</div>
<div class="noteText">
    機械学習において、単にデータが「ある」というだけでは不十分なケースもあります。つまりこの場合は「十分にあるか」という点が重要です。データ数が十分といえるかどうかは、適用する問題の難易度やデータセットの質により大きく異なります。とりわけ、画像データ分類など入力するデータが大きいケースでは、それぞれのクラス（分類する対象）のデータが数千から数万単位で必要であると言われています。近年では、インターネット上の情報であれば比較的かんたんに大量のデータを確保できます。またゲームなど、何度もくり返し試行することが可能な問題もデータ数の確保がしやすいため、機械学習の得意分野と言ってよいでしょう。一方で、データの入手がオフラインとなってしまう分野や、そもそもあまりひんぱんに起こらない現象を扱う分野では、データ数の少なさが学習のボトルネックとなる場合があります。
</div><div class="noteHeading">
    メモ - 10 得意な分野、苦手な分野 > ページ75 ·位置556
</div>
<div class="noteText">
    仕事でデータサイエンスを取り扱う場合、「データが十分にない」ケースも多々見られるとTwitterではよく見かけますね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 10 得意な分野、苦手な分野 > ページ76 ·位置566
</div>
<div class="noteText">
    機械学習は、学習データを入力したときの出力が正答に近い値になるよう、モデルを自動で最適化（学習）するアルゴリズムです（教師あり学習）。つまり、必ずしも人間の思考のように推論が進むとは限らず、その過程を見ても根拠がわからないことが多いのです。そのため機械学習で病気を診断したとして、「あなたは〇〇という病気である可能性が高いです。しかし根拠はわかりません」という結論が出るかもしれません。これでは当然、患者を納得させることもできないでしょう。このように、根拠が重要となる推論が必要な分野において、機械学習のみで結論を出すことは難しいのです。
</div><div class="noteHeading">
    メモ - 10 得意な分野、苦手な分野 > ページ76 ·位置572
</div>
<div class="noteText">
    この分野分けのポイントは大事にしなきゃいけなさそうだね。
</div><div class="sectionHeading">
    3章 機械学習のプロセスとコア技術
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 12 機械学習の基本ワークフロー > ページ87 ·位置631
</div>
<div class="noteText">
    機械学習システムの開発は、通常のシステム開発に比べ、アルゴリズムの選定や機械学習の性能向上のために試行錯誤が多く、プロセス間をまたいだ手戻り（前段階に戻ってやり直すこと）が発生しやすいといえます。そのため各プロセスに費やす時間を適切に管理することが重要になります。ただしここで重要なことは、「解決したい問題がそもそも機械学習に向いているのかどうか」を事前に見極めておくことです。機械学習により得られる予測は必ずしも正しいわけではありません。問題によっては Section 04で取り上げたエキスパートシステムを利用したほうが効率がよい場合などもあるため、まずは「他のアプローチはないか」の検討を必ず行いましょう。
</div><div class="noteHeading">
    メモ - 12 機械学習の基本ワークフロー > ページ88 ·位置637
</div>
<div class="noteText">
    機械学習によるアプローチも「手段に過ぎない」ことを忘れてはいけない。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 13 データの収集 > ページ93 ·位置693
</div>
<div class="noteText">
    外部からデータを取得する場合と異なり自分でデータを記録する場合には、「データの蓄積にかかる期間」も考慮に入れる必要があります。例として、ある顧客がサービスを解約する可能性を機械学習で予測することを考えてみましょう。年間数件しか解約が発生しない場合には、 5年間収集したとしてもせいぜい数十件程度しか集まりません。
</div><div class="noteHeading">
    メモ - 13 データの収集 > ページ94 ·位置696
</div>
<div class="noteText">
    大事な部分かもしれない。ビッグデータと呼ばれる分野ならなおさら。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 13 データの収集 > ページ97 ·位置721
</div>
<div class="noteText">
    Web APIは、一般的には Webサービスの提供者が用意したアプリケーションです。 Web APIを利用することで、その Webサービスで扱われているデータを取得できます。取得できるデータは Webサービスによってさまざまです。たとえば Facebookや Twitterなどの SNSの Web APIでは、 SNS内の投稿やユーザー情報、トレンドなどの情報を、また大手インターネット通販サービスの Amazonの Web APIでは商品情報や売れ筋などの情報を取得できます。
</div><div class="noteHeading">
    メモ - 13 データの収集 > ページ97 ·位置725
</div>
<div class="noteText">
    松尾さんがこの辺りめちゃくちゃ勝手に使っていると同時に、やれることは本当に多そうなんだよな。「Webの分野だからやらない」という思考よりも「その中でもどの辺なら活かせそうなのか」の思考でやっていく方が良さそうだと感じるぞ。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 15 モデルの作成と学習 > ページ106 ·位置787
</div>
<div class="noteText">
    機械学習におけるモデルとは、入力されたデータからある出力（入力データに対する分類や予測）を導き出すための数理モデルのことです。このことは、「何かを入れると何かが出てくる箱」をイメージすると、わかりやすくなります。箱はそれぞれ大きさや入口の形が異なっており、それによってどんなものを入れられるかが決まります。同様に、箱から出てくるものの形も決まっていると考えてください。そのようなモデルを作成するためには、入出力するデータがどのようなものであるかを最初に決定しなければなりません。ちなみにこの「箱」は、数学における「関数」にあたります。機械学習において、アルゴリズムの中で行われていることは関数の計算なのです。
</div><div class="noteHeading">
    メモ - 15 モデルの作成と学習 > ページ107 ·位置794
</div>
<div class="noteText">
    ソニーの応募条件で求められているの、絶対にここだよな。少しイメージが湧いてきたかも。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 15 モデルの作成と学習 > ページ109 ·位置810
</div>
<div class="noteText">
    それでは実際に、入力データにフィットするような直線モデルのパラメータについて考えていきましょう。このようなパラメータを探すには、どのような処理が適切なのでしょうか。まず、モデルに適当なパラメータを入れて出力を計算します。すると、必ず出力データと正答データの間に大きな差が見られます。この差のことを、訓練誤差といいます。学習においては、この訓練誤差が小さくなるようにどんどんパラメータを更新していくのです。たとえば傾きに関しては、下図のようにパラメータを更新するごとにデータに沿うように直線の傾きが変化していくことになります。
</div><div class="noteHeading">
    メモ - 15 モデルの作成と学習 > ページ109 ·位置816
</div>
<div class="noteText">
    こうやって誤差を小さくして適切な形（数値や式）を割り出すのって、AHCみたいなヒューリスティック系のやつに感覚近いのかな？
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 16 バッチ学習とオンライン学習 > ページ114 ·位置862
</div>
<div class="noteText">
    オンライン学習の欠点は、異常なデータが入力されるとモデルの予測能力が低くなることです。これは、新しく与えられたデータは例外なく正しい分類としてパラメータを更新するためです。これを防ぐには、異常検出アルゴリズムを使うなどして異常なデータの入力を監視する必要があります。
</div><div class="noteHeading">
    メモ - 16 バッチ学習とオンライン学習 > ページ115 ·位置865
</div>
<div class="noteText">
    こういった修正方法の特徴はしっかり覚える必要があるね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 17 テストデータによる予測結果の検証 > ページ117 ·位置884
</div>
<div class="noteText">
    汎化性能を検証するうえで大事なことは、「学習に利用したデータを検証に使わない」ということです。そのためには、学習データとは別に検証するためのデータを分けて残しておく必要があります。この検証用のデータを、テスト（検証・評価）データと呼びます。学習データとテストデータをわけることで、学習したモデルが未知のデータに対してどのくらいの性能があるのかを検証することができ、そこで初めてそのモデルの汎化性能を評価できます。作成したモデルで期待される性能がわからないことは、モデルの信頼性を保証できないということであり、アルゴリズムを実際に活用する上で大きな支障となるのです。
</div><div class="noteHeading">
    メモ - 17 テストデータによる予測結果の検証 > ページ118 ·位置890
</div>
<div class="noteText">
    言われてみれば「それはそうか」の気持ち。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 18 学習結果に対する評価基準 > ページ123 ·位置927
</div>
<div class="noteText">
    前のセクションでは、テストデータによって機械学習モデルの性能を検証する方法を学びました。検証においてモデルにテストデータを入力すると、それぞれのデータに対し、回帰であれば何らかの数値が、分類であればデータのラベルが予測されます。それらの結果は、回帰であればそれぞれの入力において予測と正答がどれぐらい離れているのかが値として現れます。一方の分類であれば、それぞれの入力がそれぞれ何のラベルに分類されたのか、表として集計できます。しかしこのような集計だけでは、「このモデルはどんな性能なのか」、あるいは「このモデルによる事業への利益は見込めるのか」といった重要な疑問に答えられません。
</div><div class="noteHeading">
    メモ - 18 学習結果に対する評価基準 > ページ124 ·位置933
</div>
<div class="noteText">
    当たり前のように「回帰と分類」で話を進めているけど、回帰が何かをここにきて正しく理解していない。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 18 学習結果に対する評価基準 > ページ127 ·位置961
</div>
<div class="noteText">
    2ラベル分類の混合行列の 4パターンはそれぞれ、正解が「〇」であるものを正しく「〇」と予想した回数である TP（ True Positive：真陽性）、正解が「〇」であるものを間違って「 ×」と予想した回数である FN（ False Negative：偽陰性）、正解が「 ×」であるものを間違って「〇」と予想した回数である FP（ False Positive：偽陽性）、正解が「 ×」であるものを正しく「 ×」と予想した回数である TN（ True Negative：真陰性）と呼ばれます。
</div><div class="noteHeading">
    メモ - 18 学習結果に対する評価基準 > ページ128 ·位置966
</div>
<div class="noteText">
    この表記の仕方自体がまず愉快だね。覚えたいのもそうだけど、これがどう作用するかをまた見たい。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 19 ハイパーパラメータとモデルのチューニング > ページ135 ·位置1030
</div>
<div class="noteText">
    前節の例は、多項式モデルにおける次数について考えたものでしたが、実際には利用するモデルそれぞれで決定しなければならないハイパーパラメータが多数存在します。また、前の図で示した 2次元グラフのように可視化して調整することのできない問題も多いため、ハイパーパラメータを人がチューニングすることはかなり難しいのです。そこでハイパーパラメータを決定するために、機械学習にはさまざまなオート（自動）チューニングの手法が存在します。
</div><div class="noteHeading">
    メモ - 19 ハイパーパラメータとモデルのチューニング > ページ135 ·位置1035
</div>
<div class="noteText">
    「どんなことをしているか」は理解しておくべき。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 20 能動学習 > ページ139 ·位置1053
</div>
<div class="noteText">
    機械学習（特に教師あり学習）を行うためには大量のラベル（正解）付きデータが必要ですが、ラベル付けの作業は煩雑です。そのため、教師データをやみくもに作って学習（受動学習）するのではなく、教師データの数を絞って学習する能動学習を採用すると効率的です。
</div><div class="noteHeading">
    メモ - 20 能動学習 > ページ139 ·位置1056
</div>
<div class="noteText">
    まずはここが能動学習の肝。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 21 相関と因果 > ページ145 ·位置1105
</div>
<div class="noteText">
    統計学（や機械学習）では相関関係を分析することはできますが、相関関係だけで因果関係を結論づけるには無理があります。
</div><div class="noteHeading">
    メモ - 21 相関と因果 > ページ145 ·位置1106
</div>
<div class="noteText">
    冷静に考えたらそう。言葉の意味合いをしっかり掴むこと。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 21 相関と因果 > ページ146 ·位置1119
</div>
<div class="noteText">
    データから因果分析を行う際には、下表のようなガイドラインがあります。データを分析した結果、原因とされるもの（“原因”）と結果とされるもの（“結果”）が因果関係にあると判断するためのものです。このガイドラインは、主に生物学・医学の研究で使われているものですが、機械学習や統計を利用したデータ分析に役立つ部分も多いでしょう。
</div><div class="noteHeading">
    メモ - 21 相関と因果 > ページ146 ·位置1122
</div>
<div class="noteText">
    「このうちのどれに当てはまるか」みたいなものを判別できるようにならなきゃいけないね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 22 フィードバックループ > ページ151 ·位置1158
</div>
<div class="noteText">
    機械学習ではモデルの中身がブラックボックスとなるため、振る舞いを監視することが重要です。
</div><div class="noteHeading">
    メモ - 22 フィードバックループ > ページ151 ·位置1159
</div>
<div class="noteText">
    原因というか、中での判別状況を人が見て弄って治す、みたいなレベルじゃわからないんだね。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 22 フィードバックループ > ページ151 ·位置1163
</div>
<div class="noteText">
    フィードバックループとは、システムの振る舞いが環境に影響を及ぼし、次に観測するデータが環境から影響を受けて変化してしまう現象です。この際、システムの振る舞いの変化が急であったり頻繁に起こったりする場合は、振る舞いの変化の検出は比較的かんたんです。一方、システムの振る舞いが徐々に変わっていったり、モデルの更新の頻度が低い場合には、振る舞いの変化に気づくのが遅れる場合があります。
</div><div class="noteHeading">
    メモ - 22 フィードバックループ > ページ152 ·位置1167
</div>
<div class="noteText">
    怖いねぇ…気付くのが遅れるのが一番厄介。
</div><div class="sectionHeading">
    4章 機械学習のアルゴリズム
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 23 回帰分析 > ページ156 ·位置1189
</div>
<div class="noteText">
    回帰とは、「データにもっともフィットする線を引くこと」だと思ってよいでしょう。
</div><div class="noteHeading">
    メモ - 23 回帰分析 > ページ156 ·位置1190
</div>
<div class="noteText">
    定義助かる。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 23 回帰分析 > ページ162 ·位置1236
</div>
<div class="noteText">
    単純な最小二乗法では、誤差関数を誤差の二乗和として誤差関数を最小化していました。説明変数の選び方が適切でないときに単純な最小二乗法を行うとどんどん回帰係数が大きくなってしまいますが、罰則項によってブレーキをかけることができます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 23 回帰分析 > ページ163 ·位置1249
</div>
<div class="noteText">
    線形回帰においては、 L 1正則化を用いる回帰をラッソ回帰、 L 2正則化を用いる回帰をリッジ回帰と呼びます。さらに、 L 1, L 2正則化の両方を用いたものは Elastic Net回帰と呼ばれています。
</div><div class="noteHeading">
    メモ - 23 回帰分析 > ページ163 ·位置1251
</div>
<div class="noteText">
    線形回帰。線形じゃない回帰ってなに？線形も分からんし。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ165 ·位置1266
</div>
<div class="noteText">
    Section 02で、 2店舗の派閥分類を例に、データの点をもっとも引き離すような境界線を引くことが分類だと解説しましたが、あれこそが SVMの考え方でもあるのです。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ165 ·位置1269
</div>
<div class="noteText">
    サポートベクター（ベクトル）とは、境界にもっとも近いデータの点のことです。新しいデータが入力されたときの誤判定を防ぐため、境界に近いデータであっても境界からできるだけ離すことが重要になります。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ166 ·位置1274
</div>
<div class="noteText">
    境界は、特徴量の数が 2つなら 2次元平面上の直線として、 3つなら 3次元空間上の平面として表されます。なお、 4つ以上の場合、 4次元(以上の)空間を考えなければならず、正確な図示は不可能です。そのような 4次元以上の空間における分類境界は、超平面と呼ばれます。
</div><div class="noteHeading">
    メモ - 24 サポートベクターマシン > ページ166 ·位置1277
</div>
<div class="noteText">
    超平面は初耳単語です。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ166 ·位置1278
</div>
<div class="noteText">
    SVMでは、直線・平面・超平面を境界としてデータを分離しますが、このことを線形分離といいます。しかし実際のデータにおいては、線形分離が可能なケースというのは多くないのです。
</div><div class="noteHeading">
    メモ - 24 サポートベクターマシン > ページ166 ·位置1280
</div>
<div class="noteText">
    ここでも線形。でも少しイメージ湧くからよし。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ167 ·位置1290
</div>
<div class="noteText">
    カーネル法は、派閥の境界がそもそも曲がった形（非線形）になっている場合に用います。派閥の境界が曲がっていた場合は、データから新しい特徴量を作って、うまく線形分離可能になるようにプロットしなければなりません。この作業を「線形分離可能な高次元特徴空間に写像する」と表現します。
</div><div class="noteHeading">
    メモ - 24 サポートベクターマシン > ページ168 ·位置1292
</div>
<div class="noteText">
    線じゃない形、曲線的な場合が非線形。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ168 ·位置1297
</div>
<div class="noteText">
    実際のデータでは、データに存在する特徴量から、うまく線形分離可能になるような特徴量を新たに作り出さなくてはなりません。その作り方を指定するのがカーネル関数です。カーネル関数としてはガウシアン（ RBF）カーネルや多項式カーネルなどがあります。カーネル関数を使いつつ計算量を減らす工夫は、カーネルトリックと呼ばれます。
</div><div class="noteHeading">
    メモ - 24 サポートベクターマシン > ページ168 ·位置1300
</div>
<div class="noteText">
    ガウシアンカーネル、カーネルトリック、うん、カッコイイ。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ169 ·位置1301
</div>
<div class="noteText">
    ここまで取り上げてきた SVMの長所は主に 4つです。 ①特徴量が多い場合に有効であること、 ②特徴量の数がデータの数より多い場合でも有効であること、 ③境界となる直線・平面・超平面（二次元の平面をそれ以外の次元へ一般化すること）を引く際には、境界に近い点のみを考慮すればよいため、データが多い場合もメモリを節約できること、 ④さまざまなカーネル関数を使うことができるため、多様な出力結果を得られることが挙げられます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 24 サポートベクターマシン > ページ170 ·位置1308
</div>
<div class="noteText">
    SVMは分類のほか回帰に応用される場合があります。 SVMを利用した回帰をサポートベクトル回帰（ SVR）と
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 25 決定木 > ページ171 ·位置1319
</div>
<div class="noteText">
    決定木において、条件となる部分をノード（節点）といい、一番上の条件の部分を根ノード、決定木の分類を示している末節の部分を葉ノードといいます。決定木の一部で、それ自体も木になっているものを部分木といいます。
</div><div class="noteHeading">
    メモ - 25 決定木 > ページ171 ·位置1321
</div>
<div class="noteText">
    基礎の基礎、しっかり固めて覚えて。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 25 決定木 > ページ176 ·位置1348
</div>
<div class="noteText">
    剪定は、決定木の過学習防止に効果的な方法です。多く使われる事後剪定( post-pruning)では、訓練データを使って決定木を意図的に過学習させたのち、検証データを使って性能の悪い決定木の分岐を切り取ります。
</div><div class="noteHeading">
    メモ - 25 決定木 > ページ176 ·位置1350
</div>
<div class="noteText">
    こういう作業が何よりも多くて大事よねおそらく。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 26 アンサンブル学習 > ページ178 ·位置1360
</div>
<div class="noteText">
    アンサンブル学習では、高精度のモデルを 1つ作るのではありません。精度の低いモデルをたくさん作って合体させることで、高精度のモデルを作るのが目標です。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 28 ロジスティック回帰 > ページ192 ·位置1446
</div>
<div class="noteText">
    ロジスティック回帰は教師あり学習の一種で、主に分類に利用されるアルゴリズムです。ロジスティック回帰を利用することで、ある顧客が商品を買うかどうかといった「 Yes / No」を確率として計算し推測することができます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 29 ベイジアンモデル > ページ196 ·位置1481
</div>
<div class="noteText">
    しかし、「一番ふさわしい」値だけを求めることは「その値がどれだけふさわしいのか」、「他の値はどれだけふさわしいのか」といった情報を捨てていることも意味します。単回帰は直線を引くことであるため、データが 2個あれば「一番ふさわしい」 ○と △の値を求められます。データが少ないほど分析は信頼できなくなる（ふさわしくなくなる）ことは直感でわかりますが、最尤推定ではデータが 2個のときも 1000個のときも ○と △の値が出力されるだけで、「その値がどれだけふさわしいのか」がわからないのです。
</div><div class="noteHeading">
    メモ - 29 ベイジアンモデル > ページ197 ·位置1486
</div>
<div class="noteText">
    言われてみればというか、それはそうだけど、というか…。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 29 ベイジアンモデル > ページ197 ·位置1486
</div>
<div class="noteText">
    ベイズ推定では、推定結果を「値」と「その値が推定結果である確率」のペア（分布と呼びます）で表します。これにより、値がどれだけふさわしいのかがわかります。さらに、「値がどれくらいになりそうか」という予想（事前分布）をあらかじめ立てておき、新しいデータを見て、事前分布を修正するという手続き（ベイズ更新）を踏みます。修正後の分布は事後分布といいます。予想はデータに基づかない主観的なものでもよいため、推定にデータ以外の知識を反映できるようになります。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 29 ベイジアンモデル > ページ197 ·位置1486
</div>
<div class="noteText">
    推定では、推定結果を「値」と「その値が推定結果である確率」のペア（分布と呼びます）で表します。これにより、値がどれだけふさわしいのかがわかります。さらに、「値がどれくらいになりそうか」という予想（事前分布）をあらかじめ立てておき、新しいデータを見て、事前分布を修正するという手続き（ベイズ更新）を踏みます。修正後の分布は事後分布といいます。予想はデータに基づかない主観的なものでもよいため、推定にデータ以外の知識を反映できるようになります。
</div><div class="noteHeading">
    メモ - 29 ベイジアンモデル > ページ197 ·位置1492
</div>
<div class="noteText">
    ちょうど統計Webで学んでいるところです（7月初旬現在）
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 29 ベイジアンモデル > ページ199 ·位置1506
</div>
<div class="noteText">
    ①の大きな特徴は、これらのアルゴリズムが「何らかのデータに特化して設計されているのではない」ということです。あくまでデータを学習させて予測結果を求めることに関心があるため、「データがどのように発生しているのか」について考える必要がありません。このアプローチを使うと、高度な数学の知識がなくてもかんたんなプログラムだけでデータの学習と予測を行うことができます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 29 ベイジアンモデル > ページ199 ·位置1510
</div>
<div class="noteText">
    ベイジアンモデルは ②に分類されます。このアプローチでは「データがどのように発生しているのか」というデータの発生構造（モデル）の候補をあらかじめ設計しておき、データを使ってそのモデルを推定します。そして、推定したモデルをもとに予測を行うのです。 ②では、対象とするデータに応じて考えるべきモデルを拡張したり組み合わせたりします。つまりデータの予測結果だけではなく、モデルにも関心があるわけです。このアプローチでは目的に合ったモデルを考えていくため、 ①よりも原理的に達成される性能は高くなります。
</div><div class="noteHeading">
    メモ - 29 ベイジアンモデル > ページ199 ·位置1515
</div>
<div class="noteText">
    アルゴリズム的な結果と予測だけを見るアプローチと、数学(統計？)的な要因を探るアプローチの違い、という感覚。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 29 ベイジアンモデル > ページ201 ·位置1526
</div>
<div class="noteText">
    どのようなモデルを設計するかがもっとも重要なのですが、 ②の工程は計算が複雑になるため面倒です。そこで、 ①モデルの設計に集中するための確率的プログラミング言語が用意されています。これを使うと、モデルを設計してデータを用意するだけで、データの学習からモデルの推定・予測までを行ってくれるようになります。
</div><div class="noteHeading">
    メモ - 29 ベイジアンモデル > ページ201 ·位置1529
</div>
<div class="noteText">
    非常に便利。人間の頭脳なんて頼らないのが一番なんだから。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 30 時系列分析と状態空間モデル > ページ203 ·位置1542
</div>
<div class="noteText">
    時系列分析とはその名の通り、時間ごとに取得されたデータ（時系列データ）を何らかのモデル（時系列モデル）にあてはめて説明することを指します。時間ごとに取得されたデータは、それぞれのデータ間に何かしらの関係性があります。例えば様々なりんごの品種の甘さを計測したデータがあるとします。これは時系列データではないため、例えば下のグラフのように品種 1から品種 5まで直線的に甘さが増加していたとしても偶然であり、これを元に計測し忘れた品種 3の甘さを予測することはできません。このような性質のことを、「独立」であるといいます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 30 時系列分析と状態空間モデル > ページ204 ·位置1548
</div>
<div class="noteText">
    しかしこれがある街のツバメの観測数だったらどうでしょう。去年 2羽しかいなかったのに今年いきなり 100羽になることはまずありません。つまりある年のツバメの数はそれ以前のツバメの数に少なからず影響を受けているといえます。このような性質のことを「自己相関」があるといいます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 31 k近傍(k-NN)法とk平均(k-means)法 > ページ209 ·位置1585
</div>
<div class="noteText">
    両者の違いとは、 k近傍法が主に分類に利用される教師あり学習のアルゴリズムであり、 k平均法が主にクラスタリングに利用される教師なし学習のアルゴリズムであるところです。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 31 k近傍(k-NN)法とk平均(k-means)法 > ページ211 ·位置1611
</div>
<div class="noteText">
    ここからは、 k平均法の処理の流れを解説していきます。 k近傍法が教師あり学習であったのに対し、 k平均法は教師なし学習であるため、データのラベルが既知である必要はありません。先ほどのりんごとなしに、トマトを加えた３つをクラスタに分けるアルゴリズムを例に見ていきましょう。
</div><div class="noteHeading">
    メモ - 31 k近傍(k-NN)法とk平均(k-means)法 > ページ212 ·位置1614
</div>
<div class="noteText">
    このアルゴリズムの例は非常に分かりやすかった！
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 32 次元削減と主成分分析 > ページ215 ·位置1640
</div>
<div class="noteText">
    次元削減はその名の通り、データの次元数を減らす処理のことを指します。ここでいうデータの次元とは、たとえば学生の成績データがあった場合の国語の点数、数学の点数、英語の点数……といったデータの項目数のことです。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 32 次元削減と主成分分析 > ページ219 ·位置1683
</div>
<div class="noteText">
    次元削減ではデータ量を削減しているため、当然データの本来持っていた情報の一部は失われています。この失われる情報の量のことを情報損失量といいます。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 33 最適化と遺伝的アルゴリズム > ページ221 ·位置1698
</div>
<div class="noteText">
    ある関数（目的関数）の値が最大または最小になるような解を探すことを最適化とよびます。たとえば下図の赤線のような関数の値を最大にするとき、関数の形がこのように見えればどこが最大になる解であるかが一目瞭然ですが、実際には多くの場合、目的関数の形はわかりません。そこで最適化では、試しにいくつか解を入力することによって、より目的関数の値がよい解を探し、最適解を求めるのです。
</div><div class="sectionHeading">
    5章 ディープラーニングの基礎知識
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 34 ニューラルネットワークとその歴史 > ページ230 ·位置1761
</div>
<div class="noteText">
    パーセプトロンは単一のニューロンをモデル化したもの
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 34 ニューラルネットワークとその歴史 > ページ231 ·位置1769
</div>
<div class="noteText">
    パーセプトロンをその後にもつなげていくと、入力層と出力層の間に隠れ層を持ったニューラルネットワークができます。入力層と出力層は直接観察できるのに対し、隠れ層は直接観察できず、文字通り「隠れ」ています。下図のニューラルネットワークは、隠れ層 1層を持った 2層のニューラルネットワークです。
</div><div class="noteHeading">
    メモ - 34 ニューラルネットワークとその歴史 > ページ231 ·位置1773
</div>
<div class="noteText">
    ここの「隠れ層」がディープラーニングのミソ。大事なポイント。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 34 ニューラルネットワークとその歴史 > ページ231 ·位置1774
</div>
<div class="noteText">
    なお、層の数を数える際は、入力層を除外し、「エッジのネットワークとノードにつき 1層」と考えるとよいでしょう。入力層 →隠れ層においては、入力層の各ノードは隠れ層の全てのノードと結合しています（隠れ層 →出力層においても同様）。このような層を全結合層といいます。この隠れ層をさらに何個も増やしていくと、ディープニューラルネットワークが完成します。隠れ層や隠れ層のノードの数を増やしたことでパラメータ（重み w）の数が多くなるため、より複雑な出力を行えます。一方、パラメータの数が多くなれば、学習にはより多くのデータが必要となり、過学習も起こりやすくなります。それを防ぐために使われるニューラルネットワーク特有の手法が、ドロップアウトです。ドロップアウトとは、一定の確率でノードをないものとみなして学習を行う方法です。
</div><div class="noteHeading">
    メモ - 34 ニューラルネットワークとその歴史 > ページ232 ·位置1782
</div>
<div class="noteText">
    一気に単語が増えたので、しっかり単語の整理をすること。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 34 ニューラルネットワークとその歴史 > ページ233 ·位置1785
</div>
<div class="noteText">
    活性化関数とは、入力を重み付けした和を別の値に変形させる数式のことで、ディープラーニングにおいて非常に重要です。現在利用されている活性化関数はすべて非線形関数になっています。非線形関数とは、グラフ上で一直線ではない関数のことです。なお、線形関数とは、 y =○ x +△のような形で表され、グラフ上で一直線になる関数のことです。
</div><div class="noteHeading">
    メモ - 34 ニューラルネットワークとその歴史 > ページ233 ·位置1788
</div>
<div class="noteText">
    まずは根本的な「線形」と「非線形」の区別が大事になる。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 34 ニューラルネットワークとその歴史 > ページ234 ·位置1799
</div>
<div class="noteText">
    活性化関数が非線形である理由は、現実のデータのほとんどが非線形であるから
</div><div class="noteHeading">
    メモ - 34 ニューラルネットワークとその歴史 > ページ234 ·位置1800
</div>
<div class="noteText">
    こういった因果関係を整理することが理解への第一歩なり。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 35 ディープラーニングと画像認識 > ページ244 ·位置1884
</div>
<div class="noteText">
    画像認識とは、言い換えると「パターン認識」です。ここでのパターンとは、たとえば「赤くて丸い物体はりんごである」「緑でくびれのある物体は洋梨である」といった、物体を画像として見たときの特徴を指します。機械学習では、学習データからこういったパターンを見出せるよう訓練し、実際にデータが入ってきたときにこのパターンを当てはめることで知能を実現しています。
</div><div class="noteHeading">
    メモ - 35 ディープラーニングと画像認識 > ページ244 ·位置1888
</div>
<div class="noteText">
    ここ、定義の理解に有用な文章。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 35 ディープラーニングと画像認識 > ページ245 ·位置1893
</div>
<div class="noteText">
    従来の機械学習では、データをアルゴリズムに入力する前に、データのどこに注目すればよいのかを人間が指定してその値（特徴量）を抽出しなければいけませんでした（ Section 09参照）。そのため、画像のような多次元で複雑なデータに対しては、人間が有効な特徴量を設定することが難しかったのです。しかしディープラーニングでは、適切な特徴量を学習の過程で自動的に探し出せるため、人間が気付かないデータのパターンの特徴を利用した処理が可能なのです。
</div><div class="noteHeading">
    メモ - 35 ディープラーニングと画像認識 > ページ245 ·位置1897
</div>
<div class="noteText">
    画像認識で機械学習よりディープラーニングがより良い結果を残し、発展するようになった理由。
</div><div class="sectionHeading">
    8章 システム開発と開発環境
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 49 ディープラーニングのフレームワーク > ページ340 ·位置2692
</div>
<div class="noteText">
    数あるディープラーニングライブラリの中で、圧倒的な人気を誇るのが Googleの開発する TensorFlowです。ユーザー数が非常に多いため、公式・非公式を問わず使い方やエラーの対処方法を解説した記事をインターネット上で見付けることができます。エラーメッセージを検索するとすぐに解決方法が出てくるため、安心して使えます。
</div><div class="noteHeading">
    メモ - 49 ディープラーニングのフレームワーク > ページ340 ·位置2695
</div>
<div class="noteText">
    シェアが多いことは非常に助かるね。わからないことだらけだから。
</div><div class="noteHeading">
    ハイライト(<span class="highlight_yellow">イエロー</span>) - 51 機械学習サービス > ページ354 ·位置2799
</div>
<div class="noteText">
    すでに確認してきたように、機械学習にはモデルの選定やハイパーパラメータのチューニングにある程度の知識が必要となります。また機械学習で精度のよい予測結果を得るためには、ほとんどの場合良質な大量のデータも必要です。加えて、モデルや処理するデータ量が多いなど計算負荷が高い場合には、計算資源も重要となります。これらにかかる労力やコストを削減するために利用したいのが、企業が提供している機械学習サービスです。機械学習サービスでは、企業が所有している学習済みの機械学習モデルを利用できます。
</div>