---
tags:
  - 📚Book/Techs
title: 4297134217 9784297134211
aliases:
  - Pythonで学ぶはじめてのデータサイエンス-吉田雅裕
author:
  - 吉田雅裕
publisher: 
publish: 2023-04-15
total: 0
isbn: 4297134217 9784297134211
cover: http://books.google.com/books/content?id=5syqzwEACAAJ&printsec=frontcover&img=1&zoom=1&source=gbs_api
created: 2025-09-02 20:44:20
updated: 2025-09-02 20:44:20
source: Kindle
---
![cover|150](http://books.google.com/books/content?id=5syqzwEACAAJ&printsec=frontcover&img=1&zoom=1&source=gbs_api)
# Pythonで学ぶはじめてのデータサイエンス
author: [吉田雅裕]
publisher: 
publish: 2023-04-15
Started: 2025-08-10
Finished: 2025-08-29
## 要約
本書は、AIおよびデータサイエンスの基礎から応用に至るまで、実務上有用な知識を体系的に解説するものである。まず、公開されているデータのみでは実際に価値あるAIを構築することは難しく、独自に収集したデータを組み合わせることの重要性が強調されている。PythonやGoogle Colab等の技術的解説は簡素であるが、データベース設計や前処理として正規化やクレンジングが出発点となる。具体的には、データ重複や不整合の排除、異常値や欠損値への適切な対応こそが、分析の大半を占める作業であると説かれている。

統計的検定に関しては、単なる集計にとどまらず、数学的根拠に基づき差異の有意性を評価する手法が丁寧に説明されている。t検定やクロス集計後の独立性検定などの事例を通じて、scipyやnumpy等のライブラリを活用した実装の手軽さにも触れられている。機械学習分野においては、回帰分析による数値予測、決定木やアンサンブル学習による分類、さらに汎化や過学習といった重要概念が具体例とともに披歴されている。

加えて、クラスタリングは階層型と非階層型に分類され、K平均法やエルボー法など、現場で頻用されるアルゴリズムの特徴と留意点が示される。連関分析においては、Aprioriアルゴリズムによる規則抽出、支持度・確信度・リフト値といった指標の運用、また協調フィルタリングを用いたレコメンドAIの原理まで解説が及ぶ。時系列データは、トレンド・周期・季節性・ノイズという四つの観点から整理される。画像処理AIについても、分類・物体検知・セマンティックセグメンテーションなど代表的手法の基本が平易に紹介されている。

総じて本書は、技術の根幹となる考え方に重きを置きつつ、Pythonと主要ライブラリを用いた実装例を交え、個人開発や実務でデータ活用を志す者への導入書として位置づけられる。
## 読書メモ
- 第1章：本当に概要だけなのでスキップ
- 第2章：とりあえずpythonの基礎だけをみるようなので、適当なところでスキップ。Google Colabの説明も適当なので、あまり得るものはなし。
- 第3章
	- インターネットからダウンロードできる無料の公開データは大量に用意されているものの、ビジネスとして価値の高いAIを構築することはできない。アンケート調査などを行なって独自に収集したデータと組み合わせることが重要。
	- Kaggleで公開されているデータや、他の人が加工したデータ、またどういう意図を持ってどういう加工をしたかが全て記載されている。私の視点からすれば、NBAのスタッツは楽しんでみれると思っている。
	- アンケートを取る際の質問文を作る場合は、「一つの質問文の中で1つのことを尋ねる」ことが大事。2つのことを尋ねる「ダブルバーレル質問」は避けなければならない。
- 第4章
	- 正規化とは、データの重複をなくしデータの整合性を保つためのRDBの設計方針、あるいは作業のこと。複雑で非効率にも見えるが、後からデータを追加したり更新したりするときに容易になる。
	- とりあえず感覚で正規化時の作業を整理。
		- `Join`：テーブルの結合, `pd.merge`とかで行う
		- `sort_values`：並び替え
		- `reset_index`：通し番号を振り直す
	- データクレンジングを行うことで、生データの異常値や欠損値を取り除く。これで綺麗なデータにすることで、分析可能な状態になる。データサイエンスの全肯定の80％以上はデータクレンジングとも言われる。
	- 感覚でデータクレンジングの作業を整理。
		- `loc`：おそらくlocationのこと、代入かな？
		- `replace`：文字の置換
		- `mode()`：最頻値を調べる関数。どうしようもない欠損値を、最頻値で埋めたりもできるよ。
		- `mean()`：平均値を調べる関数。こちらも欠損値を埋めるのに使いました。
		- `fillna`：欠損値に値を指定して入力。
- 第5章：数学の基礎オブ基礎って感じなのでスキップ。
- 第6章
	- 統計を用いてデータを集計した後、最終的にどのように数学的に検定結果を導き出すのかを実践。これを「統計的検定」と呼ぶ。ここが最終的に数学的知識で検定を使用する場面。
	- numpy のほかに数値解析ライブラリである scipyとやらを使用。
- 第7章
	- 異なる2つのデータの平均値を比較することを「平均値の差の検定」または「$t$ 検定」という。
	- titanicのデータから男性と女性の平均、標準偏差をまず比較。当然ながらこれでは正しい比較かわからないため、ここから有意さが取れるかを確認していく。
	- 「$p$ 値」を求めて平均値の差の検定を行った。有意水準 $α$ で棄却域を設定し、対立仮説と帰無仮説のどちらを取るかを決められる。pythonでは1行で書けてとても便利。
	- アンケート調査の内容を質的データと量的データに分類して集計する。単純集計とクロス集計で違う結果も得られる。クロス集計でまとめた分析結果に統計的有意さがあるかを確認する方法が「独立性の検定」と呼ばれる。
	- 関係性を調べたい二つの項目に関して、関係がなければ「独立」、あれば「関連」と呼び、ともに独立していれば「帰無仮説」、関連していれば「対立仮説」として検定を行う。
	- ここで独立性の検定を行うが、カイ2乗値の理論値を求める必要がある。ここでscipyを使う。カイ2乗値については要復習。イェーツの連続修正も同様に。
- 第8章
	- ソフトウェアの性能を測る指標として「スループット」「レスポンスタイム」「リソース」がある。優れたソフトウェアは、高いスループットと短いレスポンスタイムを、より少ないリソースで達成する。そしてその性能を高めるためにアルゴリズムを工夫する。
	- 単純なもので言えば全探索。素直な全探索（線形探索）だと理論上の最大値が大変なことになるが、二分探索なら簡単にできるような話。
- 第9章
	- 教師あり学習により分析した結果の出力方法の一つが「回帰」であり、「どのカテゴリか」ではなく「どのような数値になるか」を出力する。出力される数値は「連続値となる。」
	- 量的データの関係性を読み解くことを「回帰分析」と呼ぶ。2つの量的データを要因と結果の関係式として定式化し、求められた一次式の直線を「回帰直線」と呼ぶ。回帰直線と偏差の関係性を考えた時に、正負による誤差を除く方法の一つに「最小二乗法」がある。
	- 教師あり学習でAIを作る際に、学習データに過剰に適合し、汎用性がない状態に陥る現象を「過学習」と呼ぶ。逆に未知のデータに対しても適用できるようにすることを「汎化」と呼ぶ。
	- 回帰分析のうち、1種類の説明変数を用いて目的変数を予測することを「単変量解析」と呼び、その一種が「単回帰分析」である。
	- scikit-learnはpythonの機械学習ライブラリ。直線的に予測する解析の場合は `LinearRevression()`などを利用する。
	- 学習データとテストデータに対する予測結果の予測精度を示す数値は「決定係数」と呼ばれ、1に近いほど予測精度が高く、また0.6を下回る回帰AIはあまり使い物にならない。
	- 単回帰分析に対し、複数の説明変数で予測する手法を「重回帰分析」といい、単変量回帰分析に対して「多変量解析」と呼ばれる。これによりAIの予測精度を向上させることができる。また重回帰分析では複数の説明変数の単位が異なるため、標準化させる必要がある。
- 第10章
	- 分類の特徴は、あるデータが「どのカテゴリに属するか」という結果を出力すること。さまざまな入力データをもとに離散値を出力する。2種類の分類なら「2値分類」、それ以上なら「多値分類」と呼ばれる。
	- 決定木は、データにお基づいた意思決定を行うときに用いられる分析手法のこと。最終的に樹形図のような分析結果を出力する。決定木による分類で意思決定のプロセスの解釈が容易になることが利点。
	- 決定木の中で埋められていない点においては、「情報利得」という値を用いて要素を決定する。最終的なゴールを決めるときに、分ける必要がないような要素を選択肢として用意する場合は情報利得が「小さい」とされ、逆によく分けられて意思決定に役立つような場合は情報利得が「大きい」とされる。
	- 決定木が過学習をしないように汎化性能を高めることを「枝刈り」と呼ぶ。判断基準を付け加えていき複雑に分岐しながら、最終的に完全に分類できる状態になることがある。しかし、この場合は「未知のデータに対応できない」という過学習の状態に陥りやすくなる。そこで、ある程度成長したら止める「剪定（枝刈り）」が必要となる。
	- 多変量解析であっても、決定機であれば標準化は必要ない。
	- 決定木に表すことで、複数の説明変数のうち「どの説明変数が予測に役立ったのか」を表す「特徴量重要度」を可視化できることが大きな利点。
	- 複数のAIを連携させて決定機の分類精度を向上させることを「アンサンブル学習」という。分類精度を向上させるほか、過学習の発生を抑える、データ不足でも効率よく進められるなどのメリットがある。
	- 「バギング」とはアンサンブル学習の手法の一つ。複数のAIを独立に学習させたあと、アンサンブルする形態。元の学習データからランダムに復元抽出(重複を許して抽出)する操作を「ブートストラップ」という。
	- 複数作成したAIは、すべてのデータを使って学習したAIと比較して、予測精度が若干低下するので「弱学習器」と呼ばれる。バギングでは複数の弱学習器の予測結果の多数決をとる。
- 第11章
	- クラスタリングとはデータ間の類似度に基づいてグループ分けする手法のこと。分類と異なり、正解のないデータから行う教師なし学習の分類で行う手法である。
	- クラスタリングは「階層的」と「非階層的」の2種類の手法がある。
	- 階層的クラスタリングはクラスタの併合（データのグループ分け）の過程を表した図を作成する手法で、この図をデンドログラムと呼ぶ。データ数が多くなると複雑になり計算時間も長くなるため、データ数が少ない時に適している。
	- 階層的クラスタリングのクラス多数はデンドログラムから確認できる。またクラスタ間の距離の計算方法はいくつかあるが、ウォード法（Ward法）はその一つ。
	- 階層的クラスタリングの実行とデンドログラムの表示には、数値解析ライブラリ「scipy」と可視化ライブラリ「matplotlib」を使う。
	- 非階層クラスタリングではグループ分けの良さを表現する指標を定義し、反復的に計算して指標が最適となるグループ分けを見つける手法。クラスタ数をあらかじめ決める必要があるが、膨大なデータでも比較的計算が早く終わるので、ビジネスの現場ではこちらが主に用いられる。
	- 非階層クラスタリングの代表的なアルゴリズムに「K平均法（K-means）」がある。非階層クラスタリングを行うためのものであり、「クラスタの重心座標（平均）を用いて、指定されたクラス多数K個にグループ分けする」という方式。重心座標をランダムに設定し、少しずつ重心座標を動かすことになるので、同じデータでも計算するたびにクラスタリング結果が多少変化することがある。よって最良の結果を得るためには、重心座標を変えて何度か計算を実施するという工夫が必要になる。
	- クラスタリング結果の性能を測るためには、「歪み（Distortion）」を計算することが一般的である。K平均法では「クラスタ内誤差平方和（SSE）」を用いて歪みを数値化する。同じクラスタのデータが近くに集まっていると歪みは小さくなる。歪みの値は性能を測定するだけではなく、K平均法の最適なクラスタ数を検討する際にも有用。
	- K平均法でクラスタ数を増やして歪みが改善しなくなった時、クラスタ数を1から順に増やしながらクラスタリングを行い、その結果の歪みを折れ線グラフに表示する方法を「エルボー法」と呼ばれる。
- 第12章
	- 「連関分析（Basket Analysis）」は、オンラインショッピングのレコメンド（推薦）機能などにも使われている、関係性に関する知識を出力する教師なし学習の一種。連関分析で発見される規則のことを「連関規則（Association Rule）」と呼ぶ。正解を決める必要がない一方で、必要なデータはとても巨大になりやすい。
	- 連関規則を効率的に抽出するアルゴリズムの代表的な手法が「アプリオリ（Apriori）アルゴリズム」。支持度と確信度の2つの指標が用いられる。AとBの買い物について見ると、「 $A \cap B \div All$ 」が支持度（A→Bの支持度）、「 $A \cap B \div A$ 」が確信度（A→Bの確信度）。まず一つの要素からなる候補集合を生成し、支持度と確信度が閾値以上であるという条件でふるい分けして、頻出アイテムの集合を作り出す。続けて二個要素からなる候補集合を生成して…という手順を繰り返して、少ない計算時間で連関規則を見つけ出すのが、このアルゴリズムである。
	- そもそも確信度、支持度ともに一定の高さがないと、売れていない商品の観測になってしまう。観測できる度合いかどうかを判断するため「リフト値」を確認する必要がある。A→Bの確信度をB単体の支持度で割ることで算出できる。リフト値が1より大きければ、有効な関連規則とみなすことができる。
	- ビッグデータにこのアルゴリズムを適用すると、連関規則は膨大な量となる。最終的に抽出された連関規則が意味のあるものであるかどうかを、最終的には人間が検証する必要があることを忘れてはいけない。
	- ユーザーの行動履歴を利用するレコメンドAIに「協調フィルタリング」がある。いわゆる「好みが似ているユーザー同士」を関連づけて、おすすめの商品を提示する手法。ユーザー間の類似度を定義することが大切だが、ユーザーの「個人情報」と「行動」の2種類の類似度が主に考えられる。
	- 協調フィルタリングの主な利点として「行動履歴」さえあれば、ユーザーや商品の情報・属性を知らなくてもレコメンドできること。深い知識がいらない。
	- 協調フィルタリングでは教師あり学習の一種である「K近傍法（K-Nearest Neighbor Algorithm）」というアルゴリズムがよく用いられる。学習データを空間上にプロットし、未知のデータが得られたら、そこから近い順に任意のK個を取得し、多数決でデータが所属するクラスを推定するというもの。複雑な数式が必要ない、単純なアルゴリズム。
	- K近傍法は `from scipy.sparse import csr_matrix` と `sklearn.neighbors import NearestNeighbors` で実行できる。
- 第13章
	- 世の中の様々なデータは時間に関する情報を持っているが、そのうち「ある一定の時間間隔で定期的に測定されたデータ」を時系列データと呼び、これに対し「事象が発生したタイミングでバラバラに測定されたデータ」を点過程データと呼ばれる。
	- 時系列データは点と点の間が大きく開いていなければ、おそらく直線的に変化するであろうと仮定し補完して読み取る。その上で関係性に着目しながら変化を捉え、将来の傾向を把握することを目的としている。
	- 点過程データは仮定をおかず、測定された時間ごとに記録されたデータのため、1つ前のデータとの時間感覚がバラバラなまま読み取る。あくまでもある事象が発生する瞬間のメカニズムを分析することを目的としている。
	- 時系列データ分析において、予測したい値のことを「目的変数」と呼び、目的変数が記録された時の日時のデータを「説明変数」と呼ぶ。日時のデータには1日、1週間、1ヶ月という周期性を持っていることが重要な鍵となっている。周期性を前提として予測を行えることが、「日時」というデータが重要な役割を果たす要因となっている。
	- 時間軸でデータが変わっていく要因は4種類に分けられる
		- 「傾向変動（トレンド）」は、長期的に上昇・下降を示すもの。細かい変化ではなく、総合的な変化傾向を確認できる。例として地球温暖化による気温変化などが挙げられる。
		- 「循環変動（サイクル）」は、ある周期性を持って現れる変化を示すもの。測定期間を区切って部分的な傾向を示す。例として戦後の日本の景気の循環変動が挙げられる。
		- 「季節変動（シーズナル）」は、一定の周期ごとに繰り返される変化を示すもの。四季に限らず半年、四半期、月別なども季節変動として考える。例としてデパートの売り上げの変化傾向などが挙げられる。
		- 「不規則変動（ノイズ）」上記3つの変動要素で説明がつかない短期的な変化を示すもの。例として突然の天災などによる株価の変動などが挙げられる。
- 第14章
	- AIによる画像分析では、大量の画像データを取り込み教示あり学習のアルゴリズムを用いて、取り込んだ対象の特徴を学習させる。手描き文字の認証や顔認証で使われる。
	- 画像データは1ピクセルあたり白黒なら1バイト、カラーならRGBで3バイトの情報量となる。さらに解像度が高ければ高いほど、情報量も計算量も増加することになる。
	- 「画像分類」は画像分析AIの一つで、提示された画像がどのカテゴリに属するかを判断するもの。カテゴリごとの特徴を覚えさせる必要があるため、カテゴリ外の画像に分類させることはできない。
	- 「物体検出」は画像の中から対象物の位置や大きさ、カテゴリを四角形の領域で囲って検出することができる。対象物の位置検出が増える分、画像分類と比較して計算負荷が高くなる。
	- 画像分類や物体検知が対象物を四角形の領域で囲むことに対し、背景などが入らないように輪郭に沿って抽出する技術を「セマンティックセグメンテーション」という。対象物をピクセル単位で詳細化して出力できるので、対象物の位置と大きさの推定がより正確にできる。自動運転AIの運転制御などに役立つ。
## ハイライト/メモ